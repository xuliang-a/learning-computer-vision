# 目录
* []()
* []()
* []()

分类算法的评估方法有哪些

TP
FP
FN
TN

## 线性回归和逻辑回归的区别

## Q：什么是机器学习？
A：摘自《百面机器学习》第10页。
机器学习指计算机通过观察环境，与环境交互，在吸取信息中学习、自我更新和进步。简单地说，大多数机器学习算法可以分成训练(training)和测试(testing)两个步骤，这两个步骤可以重叠进行。训练，一般需要训练数据，就是告诉机器前人的经验，比如什么是猫、什么是狗、看到什么该停车。训练学习的结果，可以认为是机器写的程序或者存储的数据，叫模型(model)。总体上来说，训练包括有监督 (supervised learning)和无监督(unsupervised learning)两类。有监督好比有老师告诉你正确答案；无监督仅靠观察自学，机器自己在数据里找模式和特征。深度学习（deep learning）是机器学习的一种方法，它基于神经元网络，适用于音频、视频、语言理解等多个方面。

## Q：机器学习的划分？
A：
根据数据是否有标签划分：监督学习、非监督学习、半监督学习。
监督学习：已知标签的分类和回归问题都属于监督学习；非监督学习：数据的标签未知，根据数据本身的特征和某种度量，从中学习特征，这种情况下对样本的划分称为聚类，常用的方法有k-means、混合高斯模型；半监督学习：比如图像分类，耗费人力标注，成本高，使用弱标注（没有标注或不严格标注）来训练算法。

Q：为什么要对数值数据类型的特征做归一化？
A：摘自《百面机器学习》第23页。
在模型求解的过程中，若特征变量x在不同维度之间的取值范围差异很大，这就造成了在使用梯度下降优化算法时，由于维度之间的差异使得模型收敛的迭代次数增多，归一化后各个特征更新速度变得更为一致，容易更快地通过梯度下降找到最优解。
常用的归一化方法有2种：
（1）线性函数归一化：将不同维度的原始特征都映射到[0,1]范围内
归一化的X = 
（没有归一化的X减去X特征中的最小值）/（X特征的最大值与最小值的差值）
(2) 零均值归一化：将原始特征映射到均值为0，标准差为1的分布上
      归一化的X = (x-μ)/σ

Q：降维中PCA原理和LDA原理是什么？
A：从博客中总结的。
两者都属于数据降维方法。将高维度数据进行处理，去掉噪声和不重要的特征，保留下最重要的特征。降维可以利用少量的信息损失来为我们节省时间和成本。
PCA是主成分分析方法。PCA的主要思想是将n维特征映射到k维上，这k维是全新的正交特征也被称为主成分。PCA的工作就是从原始的空间中顺序地找一组相互正交的坐标轴，新的坐标轴的选择与数据本身是密切相关的。其中，第一个新坐标轴选择是原始数据中方差最大的方向，第二个新坐标轴选取是与第一个坐标轴正交的平面中使得方差最大的，第三个轴是与第1,2个轴正交的平面中方差最大的。依次类推，可以得到n个这样的坐标轴。通过这种方式获得的新的坐标轴，我们发现，大部分方差都包含在前面k个坐标轴中，后面的坐标轴所含的方差几乎为0。于是，我们可以忽略余下的坐标轴，只保留前面k个含有绝大部分方差的坐标轴。事实上，这相当于只保留包含绝大部分方差的维度特征，而忽略包含方差几乎为0的特征维度，实现对数据特征的降维。
LDA是线性判别分析方法。和PCA不同的是，PCA是基于方差进行降维，而LDA是根据类别进行降维，PCA是无监督问题，而LDA是有监督问题。LDA将数据在低维度上进行投影，投影后希望每一种类别的投影点尽可能接近，而不同类别的中心点之间的距离尽可能的大，中心思想就是最大化类间距离和最小化类内距离。
如果在不想暴漏标签的场景下可以使用PCA方法降维。
Q：准确率（Accuracy），精确率（Precision），召回率（Recall）都是什么？
A：
准确率是指分类正确样本占总样本个数的比例。但是若正负样本比例非常不均衡时，整体准确率虽高但是对占比不够高的样本类型的准确率可能会很低。可以通过选择合适的评估指标、合理划分测试集和训练集来解决。
精确率是指分类正确的正样本个数占分类器判定为正样本个数的比例。
召回率是指分类正确的正样本个数占真正的正样本个数的比例。
精确率和召回率是两个即矛盾又统一的指标，为了提高精确率分类器要尽量在更有把握时才将样本预测为正样本，但此时往往过于保守而漏掉很多没有把握的正样本，导致召回率降低。
若用户找视频，模型返回的前五个位置上的准确率非常高，但还是找不到想用的视频，就是说明模型没有将用户想找的视频都找出来，所以问题出在召回率上。为了综合评估一个排序模型的好坏，不仅要看模型在不同Top N下的Precision@N和Recall@N，而且需要绘制出P-R（Precision-Recall）曲线。

