# 目录
* [Q1-什么是机器学习？](Q1-什么是机器学习)
* [Q2-机器学习的学习方式有哪些？](#Q2-机器学习的学习方式有哪些)
* [Q3-线性回归和逻辑回归的区别？](#Q3-线性回归和逻辑回归的区别)
* [Q4-分类算法的评估方法有哪些？](#Q4-分类算法的评估方法有哪些)

# 机器学习基础

## Q1-什么是机器学习

机器学习（Machine Learning，ML）指计算机通过观察环境，与环境交互，在吸取信息中学习、自我更新和进步。

简单地说，大多数机器学习算法可以分成训练(training)和测试(testing)两个步骤，这两个步骤可以重叠进行。

训练，一般需要训练数据，就是告诉机器前人的经验，比如什么是猫、什么是狗、看到什么该停车。

训练学习的结果，可以认为是机器写的程序或者存储的数据，叫模型(model)。

总体上来说，训练包括有监督 (supervised learning)和无监督(unsupervised learning)两类。有监督好比有老师告诉你正确答案；无监督仅靠观察自学，机器自己在数据里找模式和特征。

深度学习（deep learning）是机器学习的一种方法，它基于神经元网络，适用于音频、视频、语言理解等多个方面。

> 摘自《百面机器学习》P10

对于一个任务及其表现的度量方法，设计一种算法能够提取数据中所蕴含的规律，这就叫机器学习。

> 摘自《机器学习500问》P16

## Q2-机器学习的学习方式有哪些

根据数据是否有标签划分：监督学习、非监督学习、半监督学习。

- 监督学习：已知正确标签的分类和回归问题都属于监督学习；

- 非监督学习：数据的标签未知，根据数据本身的特征和某种度量，从中学习特征，这种情况下对样本的划分称为聚类；

- 半监督学习：输入数据部分被标记，部分没有被标记，比如图像分类，耗费人力标注，成本高，使用弱标注（没有标注或不严格标注）来训练算法。

- 另外还有若监督学习，已知数据和其一一对应的弱标签，训练一个智能算法，将输入数据映射到一组更强的标签。标签的强弱指的是标签蕴含信息量的多少，比如相对于分割的标签来说，分类的标签就是弱标签。

应用场景和算法举例：

- 监督学习
  + 应用场景：分类和回归
  + 算法举例：支持向量机(Support Vector Machine, SVM)，朴素贝叶斯(Naive Bayes)，逻辑回归(Logistic Regression)，K近邻(K-Nearest Neighborhood, KNN)，决策树(Decision Tree)，随机森林(Random Forest)，AdaBoost以及线性判别分析(Linear Discriminant Analysis, LDA)等。深度学习(Deep Learning)也是大多数以监督学习的方式呈现。

- 非监督学习
  + 应用场景：聚类和关联规则学习
  + 算法举例：k-Means算法和Apriori算法

- 半监督学习
  + 应用场景：分类和回归
  + 算法举例：图论推理算法（Graph Inference）或者拉普拉斯支持向量机（Laplacian SVM）

参考《深度学习500问》

## Q3-线性回归和逻辑回归的区别

线性回归倾向于得到数值类型的结果（回归），如房价预测为12200.98 元/平方米；

逻辑回归倾向于得出逻辑层面的结果（分类），这个结果可以是：是否为垃圾邮件，是否患有癌症等等。

线性回归流程是训练出一组参数θ，对于给定的属性值自变量x，直接计算出因变量y=θ*x+c的值；

而逻辑回归在上一步计算出y'=θ*x+c的值之后，通过sigmoid函数将y'映射为y，再根据y值所在的范围区间将这个样本分到对应的类别中，得到逻辑上的判定结果，最常用于二分类。

摘自https://www.cnblogs.com/melon-king27/p/12825908.html

# Q4-分类算法的评估方法有哪些

对于二分类来说，只有正例（Positive）和负例（Negative）

True positives(TP): 被正确地划分为正例的个数；False positives(FP): 被错误地划分为正例的个数；
False negatives(FN):被错误地划分为负例的个数；True negatives(TN): 被正确地划分为负例的个数。

正例个数P = TP + FN；负例个数N = TN + FP

常用的指标有如下几种：

- 正确率（accuracy） 正确率是被分对的样本数在所有样本数中的占比，accuracy = (TP+TN)/(P+N)。

- 错误率（error rate) 错误率则与正确率相反，描述被分类器错分的比例，error rate = (FP+FN)/(P+N)，对某一个实例来说，分对与分错是互斥事件，所以accuracy =1 - error rate。

- 查准率 精度（precision） precision=TP/(TP+FP)，精度是精确性的度量，表示被分为正例的示例中实际为正例的比例。

- 查全率 召回率（recall） 召回率是覆盖面的度量，度量有多个正例被分为正例，recall=TP/(TP+FN)=TP/P，可以看到召回率与真阳性率是一样的识别，衡量分类器对正例的能力。

- 真阳性率（True Positive Rate，TPR） TPR=TP/(TP+FN)

- 假阳性率（False Positive Rate，FPR） FPR=FP/(FP+TN)


Q：准确率（Accuracy），精确率（Precision），召回率（Recall）都是什么？
A：
准确率是指分类正确样本占总样本个数的比例。但是若正负样本比例非常不均衡时，整体准确率虽高但是对占比不够高的样本类型的准确率可能会很低。可以通过选择合适的评估指标、合理划分测试集和训练集来解决。
精确率是指分类正确的正样本个数占分类器判定为正样本个数的比例。
召回率是指分类正确的正样本个数占真正的正样本个数的比例。
精确率和召回率是两个即矛盾又统一的指标，为了提高精确率分类器要尽量在更有把握时才将样本预测为正样本，但此时往往过于保守而漏掉很多没有把握的正样本，导致召回率降低。
若用户找视频，模型返回的前五个位置上的准确率非常高，但还是找不到想用的视频，就是说明模型没有将用户想找的视频都找出来，所以问题出在召回率上。为了综合评估一个排序模型的好坏，不仅要看模型在不同Top N下的Precision@N和Recall@N，而且需要绘制出P-R（Precision-Recall）曲线。

PR

ROC AUC

## Q4-在有监督机器学习中的生成模型和判别模型的不同

生成模型尝试学出这个数据是是怎么产生的，而判别模型只关心数据之间的差别。

生成模型和判别模型是解决分类问题的两类基本思路。生成模型就是根据数据学习特征x和标签y的联合概率分布P(x,y)，即特征x和标签y共同出现的概率对应的分布，然后根据贝叶斯公式来求得条件概率P(y|x)，预测条件概率最大的y。判别模型就是直接学习条件概率分布P(y|x)，即特征x出现的情况下标记y出现的概率，从而预测概率最大的y。

常见的生成模型和判别模型有:

生成：朴素贝叶斯模型、隐马尔科夫模型（HMM）、混合高斯模型、LDA等。
判别：逻辑回归、支持向量机（SVM）、条件随机场（CRF）、K近邻（KNN）、决策树、最大熵模型、一般的神经网络等

》GAN是否了解，如何通俗的讲其原理？
GAN是指生成对抗网络，它让两个网络模型相互竞争，一个叫做生成模型；另一个叫判别模型。生成模型不断捕捉训练集中真实图像的概率分布，将输入的随机噪声转变为新的样本（假数据）。而判别模型同时观察真实和造假的数据，判断这个数据到底是不是真的。生成对抗网络的训练过程是使生成模型生成的数据尽可能逼近真实数据，而同时判别模型又要尽量好地区分真实数据和造假数据。


TP
FP
FN
TN




Q：为什么要对数值数据类型的特征做归一化？
A：摘自《百面机器学习》第23页。
在模型求解的过程中，若特征变量x在不同维度之间的取值范围差异很大，这就造成了在使用梯度下降优化算法时，由于维度之间的差异使得模型收敛的迭代次数增多，归一化后各个特征更新速度变得更为一致，容易更快地通过梯度下降找到最优解。
常用的归一化方法有2种：
（1）线性函数归一化：将不同维度的原始特征都映射到[0,1]范围内
归一化的X = 
（没有归一化的X减去X特征中的最小值）/（X特征的最大值与最小值的差值）
(2) 零均值归一化：将原始特征映射到均值为0，标准差为1的分布上
      归一化的X = (x-μ)/σ

Q：降维中PCA原理和LDA原理是什么？
A：从博客中总结的。
两者都属于数据降维方法。将高维度数据进行处理，去掉噪声和不重要的特征，保留下最重要的特征。降维可以利用少量的信息损失来为我们节省时间和成本。
PCA是主成分分析方法。PCA的主要思想是将n维特征映射到k维上，这k维是全新的正交特征也被称为主成分。PCA的工作就是从原始的空间中顺序地找一组相互正交的坐标轴，新的坐标轴的选择与数据本身是密切相关的。其中，第一个新坐标轴选择是原始数据中方差最大的方向，第二个新坐标轴选取是与第一个坐标轴正交的平面中使得方差最大的，第三个轴是与第1,2个轴正交的平面中方差最大的。依次类推，可以得到n个这样的坐标轴。通过这种方式获得的新的坐标轴，我们发现，大部分方差都包含在前面k个坐标轴中，后面的坐标轴所含的方差几乎为0。于是，我们可以忽略余下的坐标轴，只保留前面k个含有绝大部分方差的坐标轴。事实上，这相当于只保留包含绝大部分方差的维度特征，而忽略包含方差几乎为0的特征维度，实现对数据特征的降维。
LDA是线性判别分析方法。和PCA不同的是，PCA是基于方差进行降维，而LDA是根据类别进行降维，PCA是无监督问题，而LDA是有监督问题。LDA将数据在低维度上进行投影，投影后希望每一种类别的投影点尽可能接近，而不同类别的中心点之间的距离尽可能的大，中心思想就是最大化类间距离和最小化类内距离。
如果在不想暴漏标签的场景下可以使用PCA方法降维。



