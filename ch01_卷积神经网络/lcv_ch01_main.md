
# 卷积神经网络

## Q1：卷积神经网络是什么和全连接层有什么区别？
在卷积神经网络出现之前，MLP多层感知机是比较常见神经网络。MLP相邻层节点通常是全连接的，也就是输入层的每个节点会与输出层每个节点相连接。与MLP不同，卷积神经网络的主要组成部分是卷积层，每个卷积层通过特定数目的卷积核与输入图像进行扫描和计算，由于卷积核的尺寸一般是要小于输入图像的尺寸，所以卷积层输出的每个节点只与输入层的部分节点相连接，称为局部连接，该特点与MLP的全连接不同。另外卷积神经网络还有另一个特征，权值共享，具体来说卷积层输出的每个节点，与输入层所连接的权值是一样的，也就是都是卷积核的参数。

参考《百面深度学习》P4-7

一个多输入通道的卷积计算图例

![](https://zh.d2l.ai/_images/conv_multi_in.svg)

摘自《动手学深度学习》

## Q2：卷积层输出尺寸和感受野怎么计算？

卷积层输出的尺寸由卷积核尺寸（$ k\times k $）、卷积核滑动步长(stride)和对原图边缘所填充(padding)的尺寸所决定。

若没有步长s和填充p，有

$ \mbox{输出边长} = \mbox{输入边长}  - k + 1 $

若宽或高的两侧一共填充p(注意是一共填充还是分别填充)，没有步长s，有

$ \mbox{输出边长}  = \mbox{输入边长} - k + p + 1 $

若宽或高的两侧分别填充p，步长s，有

$ \mbox{输出边长} = (\mbox{输入边长}- k + 2p + s) / s = (\mbox{输入边长} - k + 2p)/s + 1 $，若是小数则取下限

若输入图像为$3\times 3$, 核为$2\times 2$,宽或高的两侧分别填充1，在高和宽上步幅分别为3和2，有

$ [(3 - 2 + 2)/3 + 1] \times [{(3 - 2 + 2)/2 + 1}] = 2\times 2 $

该计算的图例

![](https://zh.d2l.ai/_images/conv_stride.svg)

参考《动手学深度学习》

如果将填充p设置为小于等于滤波器尺寸一半的最大整数，那么？？？这个地方需要讨论一下

感受野，也就是卷积结果对应输入的区域尺寸，其实就是反过来求解卷积层的输入尺寸

$ \mbox{卷积层的输入尺寸（感受野）} = (\mbox{卷积层输出尺寸}-1)\times s + k - 2p  $

根据这个公式可以从后向前计算感受野，向前一层一层计算就可以计算到在原始图片上对应的感受野了。

## Q3：卷积层参数数量和计算量怎么计算？

卷积层的参数量，取决于卷积核的个数和该卷积核的参数量，若卷积核大小为$k_w\times k_h$, 输入特征图通道数为$c^i$ ,输出特征图通道数（卷积核个数）为$c^o$，则
参数量 = $c^o \times c^i \times k_w\times k_h$

卷积层的计算量，取决于卷积核在每个滑到的窗口的计算量和滑动次数，在每个滑动窗内计算量约为$c^i \times k_w\times k_h$，卷积核滑动次数就是输出特征图的数据个数，即$c^o \times o_w\times o_h$, $o_w$和$o_h$分别是输出的宽度和长度，总计算量 = $c^o \times o_w\times o_h \times c^i \times k_w\times k_h$

参考《百面深度学习》P10

## Q6：什么是池化层，有哪些池化类型？

池化（pooling）层又称为降采样层(Downsampling Layer)，它的作有缓解卷积层对位置的过度敏感性、降低网络参数和防止过拟合的作用。

池化操作可以降低图像维度的原因，本质上是因为图像具有一种“静态性”的属性，这个意思是说在一个图像区域有用的特征极有可能在另一个区域同样有用。

池化类型如下：

|                  池化类型                   |                      示意图                       | 作用                                                         |
| :-----------------------------------------: | :-----------------------------------------------: | :----------------------------------------------------------- |
|          一般池化(General Pooling)          |   ![max_pooling](https://github.com/scutan90/DeepLearning-500-questions/blob/master/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(CNN)/img/ch5/general_pooling.png)   | 通常包括最大池化(Max Pooling)和平均池化(Mean Pooling)。以最大池化为例，池化范围$(2\times2)$和滑窗步长$(stride=2)$ 相同，仅提取一次相同区域的范化特征。 |
|        重叠池化(Overlapping Pooling)        | ![overlap_pooling](https://github.com/scutan90/DeepLearning-500-questions/blob/master/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(CNN)/img/ch5/overlap_pooling.png | 与一般池化操作相同，但是池化范围$P_{size}$与滑窗步长$stride$关系为$P_{size}>stride$，同一区域内的像素特征可以参与多次滑窗提取，得到的特征表达能力更强，但计算量更大。 |
| 空间金字塔池化$^*$(Spatial Pyramid Pooling) | ![spatial_pooling](https://github.com/scutan90/DeepLearning-500-questions/blob/master/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(CNN)/img/ch5/spatial_pooling.png) | 在进行多尺度目标的训练时，卷积层允许输入的图像特征尺度是可变的，紧接的池化层若采用一般的池化方法会使得不同的输入特征输出相应变化尺度的特征，而卷积神经网络中最后的全连接层则无法对可变尺度进行运算，因此需要对不同尺度的输出特征采样到相同输出尺度。 |

参考《深度学习500问》、《动手学深度学习》

## Q5：卷积层和池化层有什么区别？

卷积层核池化层在结构上具有一定的相似性，都是对感受域内的特征进行提取，并且根据填充步长设置获取到不同维度的输出，但是其内在操作是有本质区别的

需要注意的是在处理多通道输入数据时，池化层对每个输入通道分别池化，而不是像卷积层那样将各通道的输入按通道相加。这意味着池化层的输出通道数与输入通道数相等。

其它区别：

|            |                 卷积层                 |              池化层              |
| :--------: | :------------------------------------: | :------------------------------: |
| **稳定性** | 输入特征发生细微改变时，输出结果会改变 | 感受域内的细微变化不影响输出结果 |
|  **作用**  |        感受域内提取局部关联特征        |  感受域内提取泛化特征，降低维度  |
| **参数量** |      与卷积核尺寸、卷积核个数相关      |          不引入额外参数          |


摘自《深度学习500问》


## Q6：怎么组成一个用于图像分类的基本完整的卷积神经网络？

以图像分类任务为例，在表示卷积神经网络中，一般包含5种类型的网络层次结构：

| CNN层次结构 |             输出尺寸              | 作用                                                         |
| :---------: | :-------------------------------: | :----------------------------------------------------------- |
|   输入层    |      $W_1\times H_1\times 3$      | 卷积网络的原始输入，可以是原始或预处理后的像素矩阵           |
|   卷积层    |      $W_1\times H_1\times K$      | 参数共享、局部连接，利用平移不变性从全局特征图提取局部特征   |
|   激活层    |      $W_1\times H_1\times K$      | 将卷积层的输出结果进行非线性映射                             |
|   池化层    |      $W_2\times H_2\times K$      | 进一步筛选特征，可以有效减少后续网络层次所需的参数量         |
|  全连接层   | $(W_2 \cdot H_2 \cdot K)\times C$ | 将多维特征展平为2维特征，通常低维度特征对应任务的学习目标（类别或回归值） |

> $W_1\times H_1\times 3$对应原始图像或经过预处理的像素值矩阵，3对应RGB图像的通道;$K$表示卷积层中卷积核（滤波器）的个数;$W_2\times H_2$ 为池化后特征图的尺度，在全局池化中尺度对应$1\times 1$;$(W_2 \cdot H_2 \cdot K)$是将多维特征压缩到1维之后的大小，$C$对应的则是图像类别个数。


摘自《深度学习500问》


## Q7：一个1*1的卷积层有什么作用？


## Q8：用于图像分类的卷积神经网络发展过程是怎样的？

## Q9：用于分类的卷积神经网络最后几层一般是什么层？

## Q10：有哪些变种卷积？

转置卷积、空洞卷积、可变形卷积、分组卷积

参考《百面深度学习》P11-19

