
# 卷积神经网络

## Q1：卷积神经网络是什么和全连接层有什么区别？
在卷积神经网络（Convolutional Neural Network，CNN）出现之前，多层感知机（Multi-Layer Perceptron，MLP）是比较常见神经网络。多层感知机相邻层节点通常是全连接的，也就是输入层的每个节点会与输出层每个节点相连接。与多层感知机不同，卷积神经网络的主要组成部分是卷积层，每个卷积层通过特定数目的卷积核与输入图像进行扫描计算，由于卷积核的尺寸一般是要小于输入图像的尺寸，所以卷积层输出的每个节点只与输入层的部分节点相连接，称为局部连接，该特点与多层感知机的全连接不同。另外卷积神经网络还有另一个特征，权值共享，具体来说卷积层输出的每个节点，与输入层所连接的权值是一样的，都是卷积核的内部的参数。

参考《百面深度学习》P4-7

一个多输入通道的卷积计算图例

![](https://zh.d2l.ai/_images/conv_multi_in.svg)

摘自《动手学深度学习》

## Q2：卷积层输出尺寸和感受野怎么计算？

卷积层输出的尺寸由卷积核尺寸（$ k\times k $）、卷积核滑动步长(stride)和对原图边缘所填充(padding)的尺寸所决定。

若没有步长s和填充p，有

$ \mbox{输出边长} = \mbox{输入边长}  - k + 1 $

若宽或高的两侧一共填充p(注意是一共填充还是分别填充)，没有步长s，有

$ \mbox{输出边长}  = \mbox{输入边长} - k + p + 1 $

若宽或高的两侧分别填充p，步长s，有

$ \mbox{输出边长} = (\mbox{输入边长}- k + 2p + s) / s = (\mbox{输入边长} - k + 2p)/s + 1 $，若是小数则取下限

若输入图像为$3\times 3$, 核为$2\times 2$,宽或高的两侧分别填充1，在高和宽上步幅分别为3和2，有

$ [(3 - 2 + 2)/3 + 1] \times [{(3 - 2 + 2)/2 + 1}] = 2\times 2 $

该计算的图例

![](https://zh.d2l.ai/_images/conv_stride.svg)

参考《动手学深度学习》

感受野，也就是卷积结果对应输入的区域尺寸，其实就是反过来求解卷积层的输入尺寸

$ \mbox{卷积层的输入尺寸（感受野）} = (\mbox{卷积层输出尺寸}-1)\times s + k - 2p  $

根据这个公式可以从后向前计算感受野，向前一层一层计算就可以计算到在原始图片上对应的感受野了。

## Q3：卷积层参数数量和计算量怎么计算？

卷积层的参数量，取决于卷积核的个数和该卷积核的参数量，若卷积核大小为$k_w\times k_h$, 输入特征图通道数为$c^i$ ,输出特征图通道数（卷积核个数）为$c^o$，则
参数量 = $c^o \times c^i \times k_w\times k_h$

卷积层的计算量，取决于卷积核在每个滑到的窗口的计算量和滑动次数，在每个滑动窗内计算量约为$c^i \times k_w\times k_h$，卷积核滑动次数就是输出特征图的数据个数，即$c^o \times o_w\times o_h$, $o_w$和$o_h$分别是输出的宽度和长度，总计算量 = $c^o \times o_w\times o_h \times c^i \times k_w\times k_h$

参考《百面深度学习》P10

## Q6：什么是池化层，有哪些池化类型？

池化（pooling）层又称为降采样层(Downsampling Layer)，它可以缓解卷积层对位置的过度敏感性、降低网络参数和防止过拟合。

池化操作可以降低图像维度的原因，本质上是因为图像具有一种“静态性”的属性，这个意思是说在一个图像区域有用的特征极有可能在另一个区域同样有用。

池化类型如下：

|                  池化类型                   |                      示意图                       | 作用                                                         |
| :-----------------------------------------: | :-----------------------------------------------: | :----------------------------------------------------------- |
|          一般池化(General Pooling)          |   ![max_pooling](https://github.com/scutan90/DeepLearning-500-questions/blob/master/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(CNN)/img/ch5/general_pooling.png)   | 通常包括最大池化(Max Pooling)和平均池化(Mean Pooling)。以最大池化为例，池化范围$(2\times2)$和滑窗步长$(stride=2)$ 相同，仅提取一次相同区域的范化特征。 |
|        重叠池化(Overlapping Pooling)        | ![overlap_pooling](https://github.com/scutan90/DeepLearning-500-questions/blob/master/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(CNN)/img/ch5/overlap_pooling.png) | 与一般池化操作相同，但是池化范围$P_{size}$与滑窗步长$stride$关系为$P_{size}>stride$，同一区域内的像素特征可以参与多次滑窗提取，得到的特征表达能力更强，但计算量更大。 |
| 空间金字塔池化$^*$(Spatial Pyramid Pooling) | ![spatial_pooling](https://github.com/scutan90/DeepLearning-500-questions/blob/master/ch05_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(CNN)/img/ch5/spatial_pooling.png) | 在进行多尺度目标的训练时，卷积层允许输入的图像特征尺度是可变的，紧接的池化层若采用一般的池化方法会使得不同的输入特征输出相应变化尺度的特征，而卷积神经网络中最后的全连接层则无法对可变尺度进行运算，因此需要对不同尺度的输出特征采样到相同输出尺度。 |

参考《深度学习500问》、《动手学深度学习》

## Q5：卷积层和池化层有什么区别？

卷积层核池化层在结构上具有一定的相似性，都是对感受域内的特征进行提取，并且根据填充步长设置获取到不同维度的输出。

需要注意的是在处理多通道输入数据时，池化层对每个输入通道分别池化，而不是像卷积层那样将各通道的输入按通道相加。这意味着池化层的输出通道数与输入通道数相等。

其它区别：

|            |                 卷积层                 |              池化层              |
| :--------: | :------------------------------------: | :------------------------------: |
| **稳定性** | 输入特征发生细微改变时，输出结果会改变 | 感受域内的细微变化不影响输出结果 |
|  **作用**  |        感受域内提取局部关联特征        |  感受域内提取泛化特征，降低维度  |
| **参数量** |      与卷积核尺寸、卷积核个数相关      |          不引入额外参数          |


摘自《深度学习500问》


## Q6：怎么组成一个用于图像分类的基本完整的卷积神经网络？

以图像分类任务为例，在表示卷积神经网络中，一般包含5种类型的网络层次结构：

| CNN层次结构 |             输出尺寸              | 作用                                                         |
| :---------: | :-------------------------------: | :----------------------------------------------------------- |
|   输入层    |      $W_1\times H_1\times 3$      | 卷积网络的原始输入，可以是原始或预处理后的像素矩阵           |
|   卷积层    |      $W_1\times H_1\times K$      | 参数共享、局部连接，利用平移不变性从全局特征图提取局部特征   |
|   激活层    |      $W_1\times H_1\times K$      | 将卷积层的输出结果进行非线性映射                             |
|   池化层    |      $W_2\times H_2\times K$      | 进一步筛选特征，可以有效减少后续网络层次所需的参数量         |
|  全连接层   | $(W_2 \cdot H_2 \cdot K)\times C$ | 将多维特征展平为2维特征，通常低维度特征对应任务的学习目标（类别或回归值） |

> $W_1\times H_1\times 3$对应原始图像或经过预处理的像素值矩阵，3对应RGB图像的通道;$K$表示卷积层中卷积核（滤波器）的个数;$W_2\times H_2$ 为池化后特征图的尺度，在全局池化中尺度对应$1\times 1$;$(W_2 \cdot H_2 \cdot K)$是将多维特征压缩到1维之后的大小，$C$对应的则是图像类别个数。

摘自《深度学习500问》

## Q7：一个1*1的卷积层有什么作用？

1*1卷积层首先在NIN网络中提出，后来的GoogLeNet也借鉴了该卷积。

$1\times 1 $卷积的作用主要为以下两点：
- 实现信息的跨通道交互和整合，具体来说就是实现了不同通道同一位置的信息融合。
- 对卷积核通道数进行降维和升维，减小参数量，控制模型复杂度。

摘自《深度学习500问》

使用输入通道数为3、输出通道数为2的 1×1 卷积核的计算图例

![](https://zh.d2l.ai/_images/conv_1x1.svg)

假设将通道维当作特征维，将高和宽维度上的元素当成数据样本，那么 1×1 卷积层的作用与全连接层等价。

摘自《动手学深度学习》

## Q8：用于图像分类的卷积神经网络发展过程是怎样的？

### LeNet

LeNet被广泛用于银行手写体数字识别，是现代卷积神经网络的原型。这个名字来源于LeNet论文的第一作者Yann LeCun。

![](https://static.oschina.net/uploads/space/2018/0311/012923_Ficx_876354.png)

LeNet模型若不考虑输入层，一共包含7个层。即卷积、池化、卷积、池化、卷积和2个全连接层，再简单点说就是三个卷积层和两个全连接层。

- C1是卷积层，有6个尺寸为5×5的卷积核，输入图像的尺寸是32×32，经过卷积计算后得到了6幅28×28的特征图；

- 进入S2下采样层，S2对6幅特征图进行平均池化操作，池化窗口大小为2×2，步长为2，S2层的输出为6幅14×14的特征图；

- 进入C3卷积层，由于该卷积层与S2层是局部连接的关系，所以该层分别有6个尺寸为3×5×5的卷积核、9个尺寸为4×5×5的卷积核和1个6×5×5的卷积核，经过卷积计算后得到16幅10×10的特征图，局部连接关系见表；

- 进入S4下采样层，S4对这16幅特征图进行平均池化操作，池化窗口大小为2×2，步长为2，S4层的输出为16幅5×5的特征图；

- 进入C5卷积层，有120个尺寸为16×5×5的卷积核，得到了120幅1×1的特征图，其实就是得到了一个120维的向量，这层相当于一个输出个数为120的全连接层；

- 接下来的F6是全连接层，有84个输出；

- 最后一层是输出层，有10个输出，也就是要识别的数字0到9的类别个数。

![](https://static.oschina.net/uploads/space/2018/0311/013017_pIe9_876354.png)


LeNet的卷积用法:

- $5\times 5\times $ 的卷积核，没有填充和步长，正常计算

LeNet的池化用法:

- $2\times 2$，步长为2，没有填充，使输出特征图的尺寸变为输入的一半

-------

### AlexNet

AlexNet取得了2012年的ILSVRC的竞赛冠军，由Hinton和他的学生Alex Krizhevsky设计的。也是在那年之后，更多的更深的神经网路被提出。

#### 1. 总体结构

1. 包含八个学习层：5个卷积层和3个全连接层

![](https://img-blog.csdn.net/20180829094734541?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NoZW55dXBpbmczMzM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

2. 网络太大因此将网络分布在两个GPU上，GPU间可以直接互相读写内存，而不需要通过主机内存。

3. Alex采用的并行方案基本上每个GPU放置一半的核（或神经元），还有一个额外的技巧：只在某些特定的层上进行GPU通信。这意味着，例如，第3层的核会将第2层的所有核映射作为输入。然而，第4层的核只将位于相同GPU上的第3层的核映射作为输入。

![](https://img-blog.csdn.net/20180829094603154?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NoZW55dXBpbmczMzM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

#### 2. 详解网络结构

##### 1 先不看成多个GPU

1. 输入图像$227\times 227\times 3$

2. 卷积层1（96个11*11*3的卷积核，步长是4）

计算输出卷积形状，

$\lfloor(227-11+0+4)/4 \rfloor * \lfloor(227-11+0+4)/4 \rfloor = 55 \times 55$

得出输出特征图形状为$55\times 55 \times 96$

计算输出池化形状（$3\times 3, s = 2$）

$\lfloor(55-3+0+2)/2 \rfloor * \lfloor(55-3+0+2)/2 \rfloor = 27 \times 27$

得出输出特征图形状为$27\times 27\times 96$

3. 卷积层2(256个$5 \times 5 \times 96$的卷积核，分别填充2（上下左右分别填充），步长1)

计算输出卷积形状，

$\lfloor(27-5+4+1)/1 \rfloor * \lfloor(27-5+4+1)/4 \rfloor = 27 \times 27$

得出输出特征图形状为$27\times 27 \times 256$

计算输出池化形状（$3\times 3, s = 2$）

$\lfloor(27-3+0+2)/2 \rfloor * \lfloor(27-3+0+2)/2 \rfloor = 13 \times 13$

得出输出特征图形状为$13\times 13\times 256$

4. 卷积层3（384个$3\times 3\times 256$的卷积核，填充1，步长1）

计算输出卷积形状，

$\lfloor(13-3+2+1)/1 \rfloor * \lfloor(13-3+2+1)/4 \rfloor = 13 \times 13$

得出输出特征图形状为$13\times 13 \times 384$

5. 卷积层4（384个$3\times 3\times 384$的卷积核，填充1，步长1）

计算输出卷积形状，

$\lfloor(13-3+2+1)/1 \rfloor * \lfloor(13-3+2+1)/4 \rfloor = 13 \times 13$

得出输出特征图形状为$13\times 13 \times 384$

6. 卷积层5(256个$3 \times 3 \times 384$的卷积核，分别填充1（上下左右分别填充），步长1)

计算输出卷积形状，

$\lfloor(13-3+2+1)/1 \rfloor * \lfloor(13-3+2+1)/4 \rfloor = 13 \times 13$

得出输出特征图形状为$13\times 13 \times 256$

计算输出池化形状（$3\times 3, s = 2$）

$\lfloor(13-3+0+2)/2 \rfloor * \lfloor(13-3+0+2)/2 \rfloor = 6 \times 6$

得出输出特征图形状为$6\times 6\times 256$

7. 全连接层，有4096个神经元，输出$4096\times 1$向量

8. 全连接层，有4096个神经元，输出$4096\times 1$向量

9. 全连接层，有1000个神经元，输出$1000\times 1$向量

总的来说就是卷积池化、卷积池化、卷积、卷积、卷积池化和3个全连接层

##### 2 多个GPU

博主https://blog.csdn.net/chenyuping333的图

![](https://img-blog.csdn.net/20180829094658984?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NoZW55dXBpbmczMzM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

-------

### VGG

VGG是Oxford牛津大学的Visual Geometry Group的组提出，并在2014年ILSVRC取得了着不错的效果。VGG研究的初衷是想搞清楚卷积网络深度是如何影响大规模图像分类与识别的精度和准确率的，最初是VGG-16号称非常深的卷积网络全称为（GG-Very-Deep-16 CNN），VGG在加深网络层数同时为了避免参数过多，在所有层都采用3x3的小卷积核。

#### 1. VGG-16 的总体结构

VGG的输入被设置为224x224大小的RGB图像，在训练集图像上对所有图像计算RGB均值，然后把图像作为输入传入VGG卷积网络。

VGG全连接层都为3层，根据卷积层+全连接层总数目的不同可以从VGG11 ～ VGG19，最少的VGG11有8个卷积层与3个全连接层，最多的VGG19有16个卷积层+3个全连接层。

VGG池化层都为5层，VGG网络并不是在每个卷积层后面跟上一个池化层，而是分布在不同的卷积层之下。

下图是VGG-11、13、16和19的结构图：

![](https://img-blog.csdn.net/20180831091052723?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NoZW55dXBpbmczMzM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

下图是VGG16的结构图：

![](https://img-blog.csdn.net/20180831091117872?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NoZW55dXBpbmczMzM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

#### 2. 以VGG-11的结构为例

1. 输入图像 $224\times 224\times 1$

2. VGG块1

   - 卷积层1（64个 $3\times 3\times 1$ 的卷积核，填充1（上下左右分别填充），步长1）
   
     计算卷积输出形状 $\lfloor(224-3+2+1)/1 \rfloor * \lfloor(224-3+2+1)/1 \rfloor = 224 \times 224$
     
     特征图输出形状为 $ 224 \times 224 \times 64$
     
   - 池化（$2\times 2$，步长为2）
     
     计算池化输出形状 $\lfloor(224-2+0+2)/2 \rfloor * \lfloor(224-2+0+2)/2 \rfloor = 112 \times 112$
     
     特征图输出形状为 $ 112 \times 112 \times 64$

3. VGG块2

   - 卷积层2（128个 $3\times 3\times$ 64的卷积核，填充1（上下左右分别填充），步长1）

     计算卷积输出形状 $\lfloor(112-3+2+1)/1 \rfloor * \lfloor(112-3+2+1)/1 \rfloor = 112 \times 112$
     
     特征图输出形状为 $ 112 \times 112 \times 128$
     
   - 池化（$2\times 2$，步长为2）
     
     计算池化输出形状 $\lfloor(112-2+0+2)/2 \rfloor * \lfloor(112-2+0+2)/2 \rfloor = 56 \times 56$
     
     特征图输出形状为 $ 56 \times 56 \times 128$
     
4. VGG块3

   - 卷积层3（核通道256）
   
     特征图输出形状为 $ 56 \times 56 \times 256$
     
   - 卷积层4（核通道256）
   
     特征图输出形状为 $ 56 \times 56 \times 256$
     
   - 池化
   
     特征图输出形状为 $ 28 \times 28 \times 256$

5. VGG块4

   - 卷积层5（核通道512）
   
     特征图输出形状为 $ 28 \times 28 \times 512$
     
   - 卷积层6（核通道512）
   
     特征图输出形状为 $ 28 \times 28 \times 512$
     
   - 池化
   
     特征图输出形状为 $ 14 \times 14 \times 512$
     
5. VGG块5

   - 卷积层7（核通道512）
   
     特征图输出形状为 $ 14 \times 14 \times 512$
     
   - 卷积层8（核通道512）
   
     特征图输出形状为 $ 14 \times 14 \times 512$
     
   - 池化
   
     特征图输出形状为 $ 7 \times 7 \times 512$
     
 6. 全连接层，有4096个神经元，输出 $4096\times 1$ 向量
 7. 全连接层，有4096个神经元，输出 $4096\times 1$ 向量
 8. 全连接层，有1000个神经元，输出 $1000\times 1$ 向量


VGG-11的卷积用法:

- $3\times 3$ 的卷积核，填充1（上下左右分别填充），步长1，使输出特征图尺寸与输入一致

VGG-11的池化用法:

- $2\times 2$，步长为2，没有填充，使输出特征图的尺寸变为输入的一半

#### 3. VGG 块

VGG块的组成规律是：

1.	连续使用数个相同的填充为1、窗口形状为 $3\times 3$ 的卷积层后接上一个步幅为2、窗口形状为2×2的最大池化层。

2.	卷积层保持输入的高和宽不变，而池化层则对其减半。

### NiN

### GoogLeNet

### ResNet

### DenseNet

参考博主https://blog.csdn.net/chenyuping333

## Q9：用于分类的卷积神经网络最后几层一般是什么层？

用于分类任务的卷积神经网络，其前面若干层一般是卷积层、池化层等，但网络末端一般是几层全连接层。

因为多个全连接层组合在一起就是经典的多层感知机分类模型，卷积神经网络中前面的卷积层为多层感知机提取深层的、非线性的特征。

最近，分类网络在卷积层之后、最后一层之前通常采用全局平均池化，并有如下优点：

（1）大大降低了参数量和计算量。假设输入特征图的尺寸为$w\times h$, 输出通道数为c，则全局平均池化的参数为0，计算量仅为$c \times w \times h$, 而对于k个输出单元的全连接层来说，则参数量和计算量均为$k \times c \times w \times h$

（2）具备良好的可解释性，知道特征图上哪些点对最后的分类贡献最大。

摘自《百面学深度学习》

![](https://img-blog.csdn.net/20180201141956028?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveWltaW5nc2lsZW5jZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

## Q10：有哪些变种卷积？

转置卷积、空洞卷积、可变形卷积、分组卷积

参考《百面深度学习》P11-19

## 如果将填充p设置为小于等于滤波器尺寸一半的最大整数，那么？？？这个地方需要讨论一下

## 英文词汇对照
卷积神经网络 CNN Convolutional Neural Network

多层感知机 MLP Multi-Layer Perceptron

输入层 Input Layer

输出层 Output Layer

卷积层 Convolution Layer

卷积核 Convolution Kernel

通道 Channel

步长 Stride

填充 Padding

池化 Pooling

降采样层 Downsampling Layer

最大池化 Max Pooling

平均池化 Mean Pooling

激活层 Activation Layer

全连接层 Full Connected Layer
